# DeTikZify Examples

This directory contains example scripts demonstrating how to use the **DeTikZify** framework for converting scientific figures into LaTeX/TikZ code. These scripts cover the full workflow: from data preparation and sketch augmentation to pretraining, fine-tuning, reinforcement learning, inference, and evaluation. They include both scripts from the original DeTikZify architecture and scripts designed during the course of the thesis.

---

## Overview of Files

### Data Preparation and Pretraining

- **`sketchify.py`**  
  Generates **hand-drawn-style sketches** from existing DaTikZ dataset images using a diffusion model. Not used during this thesis.

- **`pretrain.py`**  
  Pretrains the **projection layer** of the DeTikZify model on large-scale figure-caption datasets. Not used during this thesis.
  
---

### Supervised Fine-Tuning

- **`train.py`**  
  Fine-tunes a base DeTikZify model on the **DaTikZ dataset**, optionally using sketch-augmented data generated by `sketchify.py`.  
  - Supports gradient checkpointing, freezing the vision encoder, and DeepSpeed for large-scale training.  
  - Accepts pre-trained modality projectors.

- **`curriculum_train.py`**  
  Demonstrates **curriculum learning**, training the model over multiple datasets or difficulty stages.  
  - Accepts multiple DaTikZ splits (`datikz0`–`datikz3`) and progresses through them sequentially before final fine-tuning on the original dataset. 

- **`cambrian_train.py`**  
  Fine-tunes the **Cambrian variant** of DeTikZify using a concatenated vision-language model.  
  - Accepts a DeTikZify-Cambrian Model as an input which has to be created using `cambrian_create.py` in the first place.
 
- **`729v_train.py`** – Fine-tunes the **Concat_factor=1** version of the model, based on `detikzify.model.vis729`.  
  - Otherwise similar to `train.py`, showcasing how to extend DeTikZify to alternative vision configurations.

---

### Model Creation (Cambrian)

- **`cambrian_create.py`** – Creates a **Cambrian-style model** by combining multiple vision encoders (e.g. CLIP, SigLIP, DINOv2, ConvNeXt) with a LLaMA text model.  
  - Handles tokenizer setup, vision tower loading, and processor creation.  
  - Outputs a fully configured model and processor ready for fine-tuning.  
  - Useful for experimenting with multi-encoder fusion strategies.
  - Accepts a list of vision model names (e.g., `clip siglip dino`) as an input which will then be converted into DeTikZify-Cambrian-ImageProcessors.

---

### Reinforcement Learning

- **`rl.py`**  
  Fine-tunes the model using **GRPO (Group Relative Policy Optimization)** with a custom reward function based on **image similarity** between generated TikZ renderings and reference sketches.  
  - Generates multiple completions per input and uses them for reward computation.  
  - Supports freezing the vision encoder (standard) and DeepSpeed acceleration.  
  - Enables more robust model behavior beyond supervised objectives.
  - Uses this thesis' own RL learning scripts which are adapted from a former version of the TRL pipeline.

- **`grpo_new.py`**  
  A standalone script implementing **GRPO training** in detail for ablation studies.  
  - Defines a full reward pipeline using rasterization and image similarity metrics.  
  - Manages training checkpoints and gradient accumulation for distributed training.  
  - Demonstrates a full RL training loop tailored for TikZ generation tasks.
  - Uses the TRL GRPO pipeline which was adjusted after starting the work on this thesis.

---

### Inference

- **`infer.py`**  
  A lightweight script for **interactive inference** with trained models.   
  - Accepts image paths or URLs and generates TikZ code in an interactive loop.  

---

### Evaluation

- **`eval.py`**  
  Evaluates fine-tuned models on the DaTikZ test set using multiple metrics:  
  - `CrystalBLEU`, `TexEditDistance` – textual similarity  
  - `ImageSim`, `DreamSim`, `KernelInceptionDistance` – visual similarity  
  - Throughput and token efficiency metrics  
  - Supports sketch-conditioned evaluation and caching predictions.

- **`cambrian_eval.py`**  
  Similar to `eval.py` but tailored for **Cambrian models**.  
  - Adds `max_tries` for early stopping during Monte Carlo Tree Search (MCTS).  
  - Measures sampling throughput and token efficiency.  
  - Evaluates image and TikZ-level similarity across multiple candidate generations.

---

## Typical Workflow

A typical workflow using these examples might look like:

1. **Sketchify the Dataset** (Optional)
   torchrun --nproc_per_node <num_gpus> sketchify.py --path nllg/datikz-v2

2. **Create the Baseline Model**
   torchrun detikzify/model/create_baseline.py

3. **Curriculum Training** (Optional)
   torchrun --nproc_per_node <num_gpus> curriculum_train.py --base_model <checkpoint> --datikz <dataset_path> --datikz0 <dataset_epoch1> --datikz1 <dataset_epoch2> --datikz2 <dataset_epoch3> --datikz3 <dataset_epoch4> --output ./detikzify_curriculum_finetuned --curriculum_epochs 1

4. **Supervised Fine-Tuning**
   torchrun --nproc_per_node <num_gpus> train.py --base_model <checkpoint> --datikz <dataset_path> --output ./detikzify_finetuned

5. **Reinforcement Learning Post-Finetuning**
   torchrun --nproc_per_node <num_gpus> rl.py --base_model ./detikzify_finetuned --datikz <dataset_path> --output ./output_rl

6. **Evaluate Model**
   torchrun --nproc_per_node <num_gpus> eval.py --testset <test_dataset_path> --path detikzify-1B=./output_rl --output eval_results.json

---

A typical workflow using these examples for the Cambrian path might look like:

1. **Create Cambrian Model**
   python cambrian_create.py --vision_encoders clip siglip dino --output_dir ./cambrian_model

2. **Supervised Fine-Tuning**
   torchrun --nproc_per_node <num_gpus> cambrian_train.py --base_model ./cambrian_model --datikz <dataset_path> --output ./cambrian_finetuned

3. **Evaluate Model**
   torchrun --nproc_per_node <num_gpus> cambrian_eval.py --testset <dataset_path> --trainset <test_dataset_path> --path detikzify-cambrian-1B=./cambrian_finetuned --output eval_results.json --max_tries <num_tries>
